# ğŸ”¥ Insight Feed - TikTok for Insights

A mobile-first vertical scrolling feed of AI-curated insights. Swipe through interesting findings, trends, and opportunities personalized to your interests.

## What This Is

Instead of searching for information, you **discover** it. Add your interests, and AI agents automatically find and curate high-quality insights for you. Swipe through your personalized feed like TikTok - but for knowledge.

## Features

- **ğŸ¯ Interest-Based**: Add topics you care about (AI, startups, investing, etc.)
- **ğŸ¤– AI Agents**: Automatically discover and evaluate high-quality sources
- **ğŸ“± Mobile-First**: Swipe up to see next insight, like/bookmark/dismiss
- **âœ¨ Personalized**: Feed learns from your engagement
- **âš¡ Fast Discovery**: No searching, just scrolling

## Quick Start

### 1. Install Dependencies

**Backend:**
```bash
pip install -r requirements.txt
```

**Frontend:**
```bash
cd frontend
npm install
cd ..
```

### 2. Set Environment Variables

Copy the example environment file and add your API keys:
```bash
cp .env.example .env
# Edit .env and add your API keys
```

Get your API keys:
- **Anthropic**: https://console.anthropic.com/ (required for extraction)
- **Groq**: https://console.groq.com/keys (required for quality evaluation)

**Optional: Configure filtering**
- `ENABLE_POST_COLLECTION_FILTER=true` - Apply final quality filters to feed (default)
- `ENABLE_POST_COLLECTION_FILTER=false` - Show all insights that passed initial quality checks

### 3. Initialize Database

```bash
python db/init_db.py
```

### 4. Start the Application

**Terminal 1 - Backend:**
```bash
cd backend
python main.py
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
```

Visit **http://localhost:3000**

## How to Use

### Step 1: Add Your Interests

1. Visit http://localhost:3000/interests
2. Add 1+ topics you're curious about:
   - "AI agents & automation"
   - "Startup ideas & trends"
   - "Value investing"
   - "Design trends"
   - etc.

### Step 2: Generate Your Feed

```bash
# In a new terminal
python api/feed_generator.py
```

This will:
- Create AI agents for each interest
- Discover high-quality sources
- Evaluate and extract insights
- Populate your personalized feed queue

**Note**: First run takes 5-10 minutes as it creates agents and discovers sources.

### Step 3: Swipe Through Insights

1. Visit http://localhost:3000
2. Swipe up (or click next) to see insights
3. â¤ï¸ Like what resonates
4. ğŸ”– Bookmark to save for later
5. âœ• Dismiss cards you're not interested in
6. â†—ï¸ Share (coming soon)

Your engagement helps the feed learn what you find valuable!

## Project Structure

```
slm-crawl/
â”œâ”€â”€ automation/          # Core insight pipeline
â”‚   â”œâ”€â”€ topic_handler.py       # Pipeline orchestrator
â”‚   â”œâ”€â”€ discover_sources.py    # Source discovery
â”‚   â”œâ”€â”€ extraction.py          # Insight extraction (Claude)
â”‚   â”œâ”€â”€ semantic_db.py         # Vector storage + filtering
â”‚   â””â”€â”€ content_fetcher.py     # Web scraping
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py                 # FastAPI server (feed endpoints)
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â””â”€â”€ InsightFeed.tsx      # Main feed UI
â”‚       â””â”€â”€ App.tsx             # React router
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema.sql              # Database schema
â”‚   â””â”€â”€ init_db.py              # Database initialization
â”œâ”€â”€ tests/               # Testing scripts
â”‚   â”œâ”€â”€ test_quality.py         # End-to-end pipeline test
â”‚   â””â”€â”€ test_manual_extraction.py  # Debug specific URLs
â”œâ”€â”€ training_data/       # Training data collection
â”‚   â”œâ”€â”€ extraction_logs.jsonl   # LLM inputs/outputs
â”‚   â”œâ”€â”€ feedback_logs.jsonl     # User engagement
â”‚   â””â”€â”€ view_training_data.py   # View/export data
â”œâ”€â”€ docs/                # Documentation
â”‚   â”œâ”€â”€ README.md               # Documentation index
â”‚   â”œâ”€â”€ architecture.md         # System design
â”‚   â””â”€â”€ api.md                  # API reference
â”œâ”€â”€ insights.db                 # SQLite database
â”œâ”€â”€ chroma_db/                  # ChromaDB vector storage
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ .env                        # API keys (create this)
```

## Database Schema

### Key Tables

- **`user_interests`** - Topics you care about
- **`agents`** - AI agents for each interest
- **`agent_configs`** - Claude-generated search & extraction configs
- **`insights_v2`** - Discovered insights with dynamic schemas
- **`feed_queue`** - Pre-scored insights ready to show
- **`insight_engagement`** - Your likes/bookmarks/dismissals

## How It Works

```
1. You add interests
   â†“
2. Feed generator creates agents
   â†“
3. Agents discover sources (DuckDuckGo)
   â†“
4. Evaluate quality (Groq SLM)
   â†“
5. Extract insights (custom schema per agent)
   â†“
6. Score relevance to your interests
   â†“
7. Add to feed queue
   â†“
8. You swipe through in the app
```

## API Endpoints

### Interests
- `GET /api/interests` - Get user's interests
- `POST /api/interests` - Add new interest
- `DELETE /api/interests/{id}` - Remove interest

### Feed
- `GET /api/feed` - Get personalized insights (20 at a time)
- `POST /api/feed/engage` - Record engagement (like/x/bookmark/share)

### Stats
- `GET /api/stats` - Feed statistics

## Configuration

### Default Topics with Examples

The feed generator includes default example sources for common topics:

- **AI/Tech**: Anthropic blog, Simon Willison, One Useful Thing
- **Startups**: Hacker News, Indie Hackers, SaaStr
- **Investing**: Berkshire letters, Investopedia, Motley Fool

For other topics, agents use general tech/business sources. You can customize this in `api/feed_generator.py`.

### Running Agents Regularly

To keep your feed fresh, run the feed generator daily:

**Option 1: Cron Job (Mac/Linux)**
```bash
# Edit crontab
crontab -e

# Add line to run daily at 9 AM
0 9 * * * cd /path/to/slm-crawl && python api/feed_generator.py
```

**Option 2: Manual**
```bash
python api/feed_generator.py
```

## Cost Estimate

**Per Agent Creation (one-time):**
- Claude Sonnet 4: ~$0.02-0.05 per agent
- Groq (quality evaluation): ~$0.001

**Per Daily Run:**
- Groq for discovery/extraction: ~$0.01-0.05 per agent
- Almost entirely free with Groq's generous limits

**Example:** 5 interests = ~$0.10 one-time + ~$0.05/day ongoing

## Troubleshooting

### "No insights yet" even after running feed generator

**Check:**
1. Did feed generator complete successfully?
2. Are there insights in database?
   ```bash
   sqlite3 insights.db "SELECT COUNT(*) FROM insights_v2;"
   ```
3. Are they in feed queue?
   ```bash
   sqlite3 insights.db "SELECT COUNT(*) FROM feed_queue WHERE shown = 0;"
   ```

### Feed generator fails with Claude API error

**Solutions:**
- Verify `ANTHROPIC_API_KEY` is set in `.env`
- Check you're on a paid Anthropic plan (free tier very limited)
- Reduce number of interests or run one at a time

### Feed generator hangs on discovery

**Solutions:**
- DuckDuckGo might be rate-limiting
- Wait a few minutes between runs
- Reduce `max_to_evaluate` in `agent_runner.py`

### Frontend shows "Failed to load feed"

**Check:**
- Backend is running on port 8000
- CORS is enabled (already configured)
- No console errors in browser DevTools

## Development

### Reset Database
```bash
rm insights.db
python db/init_db.py
```

### View Database
```bash
sqlite3 insights.db
.tables
SELECT * FROM user_interests;
SELECT * FROM feed_queue LIMIT 5;
```

### Test API
```bash
# Check backend health
curl http://localhost:8000/

# Get feed
curl http://localhost:8000/api/feed?limit=5
```

## Documentation

See the [`docs/`](docs/) folder for comprehensive documentation:
- [Architecture](docs/architecture.md) - System design and data flow
- [API Reference](docs/api.md) - Complete API documentation
- [Testing Guide](tests/README.md) - How to run tests
- [Training Data](training_data/README.md) - SLM fine-tuning data collection

## Future Enhancements

- [ ] Better recommendation algorithm (collaborative filtering)
- [ ] Social features (share insights, follow interests)
- [ ] Bookmarks page
- [ ] Daily digest emails
- [ ] Mobile app (React Native)
- [ ] Multiple feed modes (trending, deep, contrarian)

## Tech Stack

- **Backend**: FastAPI (Python)
- **Frontend**: React + TypeScript + Vite
- **Database**: SQLite
- **AI**: Claude Sonnet 4 (config generation), Groq Llama 3.1 (discovery/extraction)
- **Search**: DuckDuckGo
- **Crawling**: Crawl4AI
- **Animations**: Framer Motion

## Credits

Built with:
- **Anthropic Claude** - Smart agent configuration
- **Groq** - Fast, affordable inference
- **Crawl4AI** - Clean web content extraction
- **Framer Motion** - Smooth swipe animations

## License

MIT

---

**Built by** [Your Name]

**Questions?** Check [`docs/`](docs/) for comprehensive documentation.
